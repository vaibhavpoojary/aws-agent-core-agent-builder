# ğŸ“˜ Agent Platform

An **AWS-native platform** for building, evaluating, deploying, and observing **LangGraph / LangChain / CrewAI-based agents**.  
It integrates with **MLflow**, **AgentCore**, and the **AWS observability stack** for an end-to-end multi-tenant agent workflow.

---

## ğŸ“‚ Folder Structure

```plaintext
agent-platform/
â”‚â”€â”€ Makefile                     # Automation shortcuts
â”‚â”€â”€ requirements.txt              # Python dependencies
â”‚â”€â”€ .env.example                  # Example environment variables
â”‚â”€â”€ README.md                     # Project documentation
â”‚
â”œâ”€â”€ build/
â”‚   â”œâ”€â”€ Dockerfile                # Container for agent
â”‚   â”œâ”€â”€ build_push.sh             # Build & push Docker to ECR
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â””â”€â”€ langgraph_agent/
â”‚   â”‚       â”œâ”€â”€ graph_stub.py     # Sample LangGraph graph
â”‚   â”‚       â”œâ”€â”€ entrypoint.py     # FastAPI wrapper for AgentCore
â”‚   â”‚       â””â”€â”€ bootstrap.sh      # Bootstrap script for AgentCore runtime
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ mlflow_utils.py       # Log/run/register agents in MLflow
â”‚   â”‚   â”œâ”€â”€ ecr_utils.py          # ECR repo & image helper
â”‚   â”‚   â”œâ”€â”€ agentcore_deploy.py   # Deploy agent tarball to AgentCore
â”‚   â”‚   â”œâ”€â”€ package_agent.py      # Package agent into tarball
â”‚   â”‚   â””â”€â”€ cloudwatch_setup.py   # Dashboards, alarms, logs
â”‚   â”‚
â”‚   â””â”€â”€ eval/
â”‚       â”œâ”€â”€ eval_pipeline.py      # Run evaluation pipelines
â”‚       â””â”€â”€ agenteval_integration.py # Hooks for agent evaluation
â”‚
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ ecr_s3_iam.yaml           # CloudFormation / CDK IaC for ECR + S3 + IAM
â”‚   â””â”€â”€ codepipeline.yaml         # CI/CD pipeline definition
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_local.sh              # Run agent locally
â”‚   â”œâ”€â”€ run_eval.sh               # Run evaluation locally
â”‚   â””â”€â”€ agentcore_cmds.sh         # AgentCore CLI shortcuts
â”‚
â””â”€â”€ tests/
    â””â”€â”€ test_agent.py             # Unit tests for agent
```

---

## ğŸš€ Quickstart

### 1ï¸âƒ£ Setup Environment
```bash
cp .env.example .env
pip install -r requirements.txt
```

### 2ï¸âƒ£ Build, Package & Deploy
Use the **Makefile** for automation:

```bash
make build      # Build Docker image
make package    # Package agent into tarball with bootstrap
make push       # Push image to ECR
make deploy     # Deploy tarball to AgentCore
```

Or run everything in one go:
```bash
make all
```

### 3ï¸âƒ£ Run Evaluation
```bash
make eval
```
This runs evaluation pipeline, logs metrics (latency, cost, accuracy) into **MLflow**, and compares across agent versions.

### 4ï¸âƒ£ Observe
```bash
make logs
```
Fetch **CloudWatch logs** and metrics.  
Dashboards & alarms are configured via `cloudwatch_setup.py`.

---

## ğŸ› ï¸ Components

- **Build Layer** â†’ Docker + SageMaker-ready packaging  
- **Evaluation Layer** â†’ Hosted MLflow integration  
- **Deployment Layer** â†’ AgentCore runtime (tarball deployment)  
- **Observability Layer** â†’ CloudWatch metrics/logs + SNS/EventBridge alerts  

---

## âœ… Best Practices

- Use **IAM least privilege** roles in `infra/ecr_s3_iam.yaml`  
- Apply **project-based tagging** for multi-tenancy (`ProjectID`, `Owner`, `Environment`)  
- Use **tarball packaging** for reproducible deployments  
- Keep **evaluation scripts modular** for multiple frameworks (LangGraph, CrewAI, etc.)  

---

## ğŸ§ª Testing

Run unit tests:
```bash
pytest tests/
```

Simulate an end-to-end flow:
```bash
make all
make eval
make logs
```
